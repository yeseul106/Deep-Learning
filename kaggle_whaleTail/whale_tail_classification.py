# -*- coding: utf-8 -*-
"""whale_tail_Classification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1MnsyXyZO_97HcdhAa9Ii5R3mntmcwnow
"""

from google.colab import drive
drive.mount('/content/gdrive')

import os
import numpy as np
import pandas as pd
import tensorflow as tf
import matplotlib.pyplot as plt
import cv2
from keras.utils import np_utils
from sklearn.model_selection import train_test_split
from keras.models import Sequential
from keras.layers import Embedding, Dense, Flatten, GlobalMaxPooling2D, Conv2D, Dropout, MaxPooling2D
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras import optimizers

np.random.seed(3)
tf.random.set_seed(3)

'''category가 몇개 있는지 확인 (고래의 종류)'''
df = pd.read_csv('/content/gdrive/My Drive/Colab Notebooks/train.csv')
# train_df.head()
category_num=len(df['Id'].unique())  #4251 (고래의 종류가 4251가지)
df['Id'].value_counts()

'''각 카테고리가 몇개씩 있는지 확인'''
group_df = df.groupby('Id').count()
group_df = group_df.reset_index()
print(group_df.sort_values(by='Image'))
# print(group_df)

'''10개 이하의 이미지를 가진 카테고리 제거'''
valid_category = []
valid_category = group_df[group_df.Image>10]
print(len(valid_category))
# print(valid_category.sort_values(by='Image'))

'''train 데이터와 test 데이터 나누기'''
df = df[df['Id'].isin(valid_category['Id'])]  #1699 rows X 2 columns
print(df)
# train_size = (int)(len(df)*0.7)
# train_df = df[:train_size]
# test_df = df[train_size:]  <== 문제점은 trian_set과 test_set에 카테고리 비율이 다름
train_df, test_df = train_test_split(df,test_size=0.3, stratify=df['Id'])

'''이미지 부풀리기'''
train_datagen = ImageDataGenerator(rescale=1./255, horizontal_flip=True, width_shift_range=0.2, height_shift_range=0.2,
                                   rotation_range=15, shear_range=0.7, fill_mode='nearest')

train_generator = train_datagen.flow_from_dataframe(
    dataframe= train_df, #데이터 프레임
    directory= '/content/gdrive/My Drive/Colab Notebooks/train', #데이터 위치
    x_col = 'Image', #파일 위치 열이름
    y_col = 'Id',  #분류할 클래스 열이름
    target_size=(200, 200),
    batch_size=30,
    class_mode='categorical')

test_datagen = ImageDataGenerator(rescale=1./255)

test_generator = test_datagen.flow_from_dataframe(
    dataframe= test_df, #데이터 프레임
    directory= '/content/gdrive/My Drive/Colab Notebooks/train',  #데이터 위치
    x_col= 'Image', #파일 위치 열이름
    y_col = 'Id', #분류할 클래스 열이름
    target_size=(200, 200),
    batch_size=30,
    class_mode='categorical')

'''전이 학습을 위해 VGG16을 나의 네트워크 전에 삽입한다!'''
from keras.applications.vgg16 import VGG16

transfer_model = VGG16(weights='imagenet', include_top=False, input_shape=(200,200,3))
transfer_model.trainable= False  #학습 시킬 것이 아니므로 False
final_model= Sequential()
final_model.add(transfer_model)
final_model.add(Dropout(0.3))
final_model.add(Flatten())
final_model.add(Dense(256,activation='relu'))
final_model.add(Dropout(0.3))
final_model.add(Dense(57, activation='softmax'))

# '''set CNN model'''
# model = Sequential()
# model.add(Conv2D(32, kernel_size=(3, 3), input_shape=(100, 100, 3), activation='relu'))
# model.add(MaxPooling2D(pool_size=(2, 2)))

# model.add(Conv2D(64,(3, 3), activation='relu'))
# model.add(MaxPooling2D(pool_size=(2, 2)))

# model.add(Dropout(0.3))
# model.add(Flatten())
# model.add(Dense(128, activation='relu'))
# model.add(Dropout(0.5))
# model.add(Dense(4251, activation='softmax'))  #출력층 노드 5개

final_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

history = final_model.fit_generator(
    train_generator,
    steps_per_epoch=10, epochs=30, validation_data= test_generator, validation_steps=4)

'''미세조정'''
transfer_model.trainable = True

from keras.optimizers import Adam

final_model.compile(optimizer=Adam(1e-5), 
              loss='categorical_crossentropy',
              metrics=['accuracy']
)

history = final_model.fit_generator(
    train_generator,
    steps_per_epoch=10, epochs=30, validation_data= test_generator, validation_steps=4)

'''model 평가하기'''
scores = final_model.evaluate_generator(
            test_generator, 
            steps = 5)

print("Test %s: %.2f%%" %(final_model.metrics_names[1], scores[1]*100))

 '''graph로 표현'''
y_vloss = history.history['val_loss']  #테스트셋 오차
y_loss = history.history['loss']  #학습셋 오차
y_vacc = history.history['val_accuracy']  #테스트셋 정확률
y_acc = history.history['accuracy']  #학습셋 정확률

x_len = np.arange(len(y_loss))
plt.plot(x_len, y_acc, marker='.', c='red', label='Trainset_acc')
plt.plot(x_len, y_vacc, marker='.', c='lightcoral', label='Testset_acc')
plt.plot(x_len, y_loss, marker='.', c='blue', label='Trainset_loss')
plt.plot(x_len, y_vloss, marker='.', c='cornflowerblue', label='Testset_loss')

plt.legend(loc='upper right')
plt.grid()
plt.xlabel('epoch')
plt.ylabel('loss/acc')
plt.show()

